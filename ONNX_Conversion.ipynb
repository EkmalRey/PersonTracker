{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d2118e",
   "metadata": {},
   "source": [
    "# ONNX Model Conversion for Mobile Deployment\n",
    "\n",
    "This notebook converts YOLO models to ONNX format optimized for mobile/CPU inference.\n",
    "\n",
    "## Benefits of ONNX:\n",
    "- **Faster inference** on CPU\n",
    "- **Smaller model size** with quantization\n",
    "- **Mobile compatibility** (Android/iOS)\n",
    "- **Cross-platform deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15064be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (8.3.85)\n",
      "Requirement already satisfied: onnx in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (1.21.0)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from onnx) (5.29.4)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ekmal\\.conda\\envs\\computervision\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f0f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: e:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\n",
      "\n",
      "Available models in e:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\\models:\n",
      "  - yolo11n.pt\n",
      "  - yolov8n-face-lindevs.pt\n",
      "  - yolov8n.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Working directory: {current_dir}\")\n",
    "\n",
    "# Check available models\n",
    "models_dir = os.path.join(current_dir, 'models')\n",
    "if os.path.exists(models_dir):\n",
    "    print(f\"\\nAvailable models in {models_dir}:\")\n",
    "    for file in os.listdir(models_dir):\n",
    "        if file.endswith('.pt'):\n",
    "            print(f\"  - {file}\")\n",
    "else:\n",
    "    print(f\"Models directory not found: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9b726",
   "metadata": {},
   "source": [
    "## 1. Convert Person Detection Model (YOLOv8n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce383812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Person Detection Model to ONNX...\n",
      "Found model at: models\\yolov8n.pt\n",
      "Ultralytics 8.3.85  Python-3.11.11 torch-2.2.2+cpu CPU (Intel Core(TM) i3-10100F 3.60GHz)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\yolov8n.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 84, 3549) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\yolov8n.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 84, 3549) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.7s, saved as 'models\\yolov8n.onnx' (12.1 MB)\n",
      "\n",
      "Export complete (3.1s)\n",
      "Results saved to \u001b[1mE:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\yolov8n.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=models\\yolov8n.onnx imgsz=416 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "‚úÖ Person detection model exported to: models\\yolov8n.onnx\n",
      "Original model size: 6.25 MB\n",
      "ONNX model size: 12.15 MB\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.7s, saved as 'models\\yolov8n.onnx' (12.1 MB)\n",
      "\n",
      "Export complete (3.1s)\n",
      "Results saved to \u001b[1mE:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\yolov8n.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=models\\yolov8n.onnx imgsz=416 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "‚úÖ Person detection model exported to: models\\yolov8n.onnx\n",
      "Original model size: 6.25 MB\n",
      "ONNX model size: 12.15 MB\n"
     ]
    }
   ],
   "source": [
    "# Load and convert person detection model\n",
    "print(\"Converting Person Detection Model to ONNX...\")\n",
    "\n",
    "# Try to find the model\n",
    "person_model_path = None\n",
    "possible_paths = [\n",
    "    'yolov8n.pt',\n",
    "    os.path.join('models', 'archive', 'yolov8n.pt'),\n",
    "    os.path.join('models', 'yolov8n.pt')\n",
    "]\n",
    "\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        person_model_path = path\n",
    "        break\n",
    "\n",
    "if person_model_path:\n",
    "    print(f\"Found model at: {person_model_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    person_model = YOLO(person_model_path)\n",
    "    \n",
    "    # Export to ONNX with mobile-optimized settings\n",
    "    onnx_path = person_model.export(\n",
    "        format='onnx',\n",
    "        imgsz=416,           # Smaller input size for mobile\n",
    "        half=False,          # Use FP32 for better CPU compatibility\n",
    "        dynamic=False,       # Fixed input size for mobile optimization\n",
    "        simplify=True,       # Simplify the model graph\n",
    "        opset=11,           # ONNX opset version (compatible with most runtimes)\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Person detection model exported to: {onnx_path}\")\n",
    "    \n",
    "    # Check model size\n",
    "    original_size = os.path.getsize(person_model_path) / (1024 * 1024)\n",
    "    onnx_size = os.path.getsize(onnx_path) / (1024 * 1024)\n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"ONNX model size: {onnx_size:.2f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Person detection model not found!\")\n",
    "    print(\"Please ensure yolov8n.pt is in the current directory or models folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf679f70",
   "metadata": {},
   "source": [
    "## 2. Convert Face Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e953e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting Face Detection Model to ONNX...\n",
      "Found face model at: models\\yolov8n-face-lindevs.pt\n",
      "Ultralytics 8.3.85  Python-3.11.11 torch-2.2.2+cpu CPU (Intel Core(TM) i3-10100F 3.60GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\yolov8n-face-lindevs.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 5, 2100) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\yolov8n-face-lindevs.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 5, 2100) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.3s, saved as 'models\\yolov8n-face-lindevs.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (1.5s)\n",
      "Results saved to \u001b[1mE:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\yolov8n-face-lindevs.onnx imgsz=320  \n",
      "Validate:        yolo val task=detect model=models\\yolov8n-face-lindevs.onnx imgsz=320 data=train/data/widerface.yaml  \n",
      "Visualize:       https://netron.app\n",
      "‚úÖ Face detection model exported to: models\\yolov8n-face-lindevs.onnx\n",
      "Original model size: 5.99 MB\n",
      "ONNX model size: 11.56 MB\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  1.3s, saved as 'models\\yolov8n-face-lindevs.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (1.5s)\n",
      "Results saved to \u001b[1mE:\\PROJECT\\Computer Vision\\Random\\PersonTracker\\MVP\\models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\yolov8n-face-lindevs.onnx imgsz=320  \n",
      "Validate:        yolo val task=detect model=models\\yolov8n-face-lindevs.onnx imgsz=320 data=train/data/widerface.yaml  \n",
      "Visualize:       https://netron.app\n",
      "‚úÖ Face detection model exported to: models\\yolov8n-face-lindevs.onnx\n",
      "Original model size: 5.99 MB\n",
      "ONNX model size: 11.56 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert face detection model\n",
    "print(\"\\nConverting Face Detection Model to ONNX...\")\n",
    "\n",
    "face_model_path = os.path.join('models', 'archive', 'yolov8n-face-lindevs.pt')\n",
    "\n",
    "if os.path.exists(face_model_path):\n",
    "    print(f\"Found face model at: {face_model_path}\")\n",
    "    \n",
    "    # Load face detection model\n",
    "    face_model = YOLO(face_model_path)\n",
    "    \n",
    "    # Export to ONNX with mobile-optimized settings\n",
    "    face_onnx_path = face_model.export(\n",
    "        format='onnx',\n",
    "        imgsz=320,           # Even smaller for face detection\n",
    "        half=False,          # Use FP32 for better CPU compatibility\n",
    "        dynamic=False,       # Fixed input size\n",
    "        simplify=True,       # Simplify the model graph\n",
    "        opset=11,           # ONNX opset version\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Face detection model exported to: {face_onnx_path}\")\n",
    "    \n",
    "    # Check model size\n",
    "    original_size = os.path.getsize(face_model_path) / (1024 * 1024)\n",
    "    onnx_size = os.path.getsize(face_onnx_path) / (1024 * 1024)\n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"ONNX model size: {onnx_size:.2f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Face detection model not found at: {face_model_path}\")\n",
    "    print(\"Please ensure the face detection model is in the models folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be90e13",
   "metadata": {},
   "source": [
    "## 3. Test ONNX Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d38c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_path, input_size, model_type=\"PyTorch\"):\n",
    "    \"\"\"Benchmark inference speed of a model\"\"\"\n",
    "    print(f\"\\nüöÄ Benchmarking {model_type} Model: {os.path.basename(model_path)}\")\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = np.random.randint(0, 255, (input_size, input_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    if model_type == \"PyTorch\":\n",
    "        # Test PyTorch model\n",
    "        model = YOLO(model_path)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            _ = model(dummy_input, verbose=False)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for i in range(10):\n",
    "            start_time = time.time()\n",
    "            _ = model(dummy_input, verbose=False)\n",
    "            times.append(time.time() - start_time)\n",
    "            \n",
    "    elif model_type == \"ONNX\":\n",
    "        # Test ONNX model\n",
    "        session = ort.InferenceSession(model_path)\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        \n",
    "        # Prepare input (ONNX expects NCHW format)\n",
    "        input_tensor = np.transpose(dummy_input, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "        input_tensor = np.expand_dims(input_tensor, axis=0)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            _ = session.run(None, {input_name: input_tensor})\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for i in range(10):\n",
    "            start_time = time.time()\n",
    "            _ = session.run(None, {input_name: input_tensor})\n",
    "            times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to milliseconds\n",
    "    fps = 1000 / avg_time\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time:.2f} ms\")\n",
    "    print(f\"Estimated FPS: {fps:.1f}\")\n",
    "    \n",
    "    return avg_time, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4124ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Performance Comparison\n",
      "==================================================\n",
      "\n",
      "üßë PERSON DETECTION MODEL COMPARISON:\n",
      "\n",
      "üöÄ Benchmarking PyTorch Model: yolov8n.pt\n",
      "Average inference time: 97.76 ms\n",
      "Estimated FPS: 10.2\n",
      "\n",
      "üöÄ Benchmarking ONNX Model: yolov8n.onnx\n",
      "Average inference time: 97.76 ms\n",
      "Estimated FPS: 10.2\n",
      "\n",
      "üöÄ Benchmarking ONNX Model: yolov8n.onnx\n",
      "Average inference time: 26.08 ms\n",
      "Estimated FPS: 38.3\n",
      "\n",
      "üéØ ONNX Speedup: 3.75x faster\n",
      "üéØ FPS Improvement: 10.2 ‚Üí 38.3 (+28.1)\n",
      "\n",
      "üë§ FACE DETECTION MODEL COMPARISON:\n",
      "\n",
      "üöÄ Benchmarking PyTorch Model: yolov8n-face-lindevs.pt\n",
      "Average inference time: 26.08 ms\n",
      "Estimated FPS: 38.3\n",
      "\n",
      "üéØ ONNX Speedup: 3.75x faster\n",
      "üéØ FPS Improvement: 10.2 ‚Üí 38.3 (+28.1)\n",
      "\n",
      "üë§ FACE DETECTION MODEL COMPARISON:\n",
      "\n",
      "üöÄ Benchmarking PyTorch Model: yolov8n-face-lindevs.pt\n",
      "Average inference time: 91.41 ms\n",
      "Estimated FPS: 10.9\n",
      "\n",
      "üöÄ Benchmarking ONNX Model: yolov8n-face-lindevs.onnx\n",
      "Average inference time: 91.41 ms\n",
      "Estimated FPS: 10.9\n",
      "\n",
      "üöÄ Benchmarking ONNX Model: yolov8n-face-lindevs.onnx\n",
      "Average inference time: 13.85 ms\n",
      "Estimated FPS: 72.2\n",
      "\n",
      "üéØ ONNX Speedup: 6.60x faster\n",
      "üéØ FPS Improvement: 10.9 ‚Üí 72.2 (+61.3)\n",
      "Average inference time: 13.85 ms\n",
      "Estimated FPS: 72.2\n",
      "\n",
      "üéØ ONNX Speedup: 6.60x faster\n",
      "üéØ FPS Improvement: 10.9 ‚Üí 72.2 (+61.3)\n"
     ]
    }
   ],
   "source": [
    "# Benchmark PyTorch vs ONNX models if they exist\n",
    "print(\"üìä Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test person detection models\n",
    "if person_model_path and os.path.exists(onnx_path):\n",
    "    print(\"\\nüßë PERSON DETECTION MODEL COMPARISON:\")\n",
    "    pytorch_time, pytorch_fps = benchmark_model(person_model_path, 416, \"PyTorch\")\n",
    "    onnx_time, onnx_fps = benchmark_model(onnx_path, 416, \"ONNX\")\n",
    "    \n",
    "    speedup = pytorch_time / onnx_time\n",
    "    print(f\"\\nüéØ ONNX Speedup: {speedup:.2f}x faster\")\n",
    "    print(f\"üéØ FPS Improvement: {pytorch_fps:.1f} ‚Üí {onnx_fps:.1f} (+{onnx_fps-pytorch_fps:.1f})\")\n",
    "\n",
    "# Test face detection models if available\n",
    "if os.path.exists(face_model_path) and 'face_onnx_path' in locals():\n",
    "    print(\"\\nüë§ FACE DETECTION MODEL COMPARISON:\")\n",
    "    pytorch_time_face, pytorch_fps_face = benchmark_model(face_model_path, 320, \"PyTorch\")\n",
    "    onnx_time_face, onnx_fps_face = benchmark_model(face_onnx_path, 320, \"ONNX\")\n",
    "    \n",
    "    speedup_face = pytorch_time_face / onnx_time_face\n",
    "    print(f\"\\nüéØ ONNX Speedup: {speedup_face:.2f}x faster\")\n",
    "    print(f\"üéØ FPS Improvement: {pytorch_fps_face:.1f} ‚Üí {onnx_fps_face:.1f} (+{onnx_fps_face-pytorch_fps_face:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b11d91",
   "metadata": {},
   "source": [
    "## 4. Model Information & Mobile Deployment Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030ea4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Analyzing: yolov8n.onnx\n",
      "----------------------------------------\n",
      "Model version: 0\n",
      "ONNX opset version: 11\n",
      "\n",
      "Input shape: [1, 3, 416, 416]\n",
      "Input type: tensor(float)\n",
      "\n",
      "Output shapes:\n",
      "  Output 0: [1, 84, 3549] (tensor(float))\n",
      "\n",
      "Model size: 12.15 MB\n",
      "\n",
      "üì± Mobile Deployment Readiness:\n",
      "  ‚úÖ Fixed input size: True\n",
      "  ‚úÖ ONNX opset ‚â§ 13: True\n",
      "  ‚úÖ Model size < 50MB: True\n",
      "  ‚úÖ FP32 precision: True\n",
      "\n",
      "üìã Analyzing: yolov8n-face-lindevs.onnx\n",
      "----------------------------------------\n",
      "Model version: 0\n",
      "ONNX opset version: 11\n",
      "\n",
      "Input shape: [1, 3, 320, 320]\n",
      "Input type: tensor(float)\n",
      "\n",
      "Output shapes:\n",
      "  Output 0: [1, 5, 2100] (tensor(float))\n",
      "\n",
      "Model size: 11.56 MB\n",
      "\n",
      "üì± Mobile Deployment Readiness:\n",
      "  ‚úÖ Fixed input size: True\n",
      "  ‚úÖ ONNX opset ‚â§ 13: True\n",
      "  ‚úÖ Model size < 50MB: True\n",
      "  ‚úÖ FP32 precision: True\n"
     ]
    }
   ],
   "source": [
    "def analyze_onnx_model(model_path):\n",
    "    \"\"\"Analyze ONNX model structure and properties\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Analyzing: {os.path.basename(model_path)}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Load model\n",
    "    model = onnx.load(model_path)\n",
    "    \n",
    "    # Model info\n",
    "    print(f\"Model version: {model.model_version}\")\n",
    "    print(f\"ONNX opset version: {model.opset_import[0].version}\")\n",
    "    \n",
    "    # Input/Output info\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    \n",
    "    print(f\"\\nInput shape: {session.get_inputs()[0].shape}\")\n",
    "    print(f\"Input type: {session.get_inputs()[0].type}\")\n",
    "    \n",
    "    print(f\"\\nOutput shapes:\")\n",
    "    for i, output in enumerate(session.get_outputs()):\n",
    "        print(f\"  Output {i}: {output.shape} ({output.type})\")\n",
    "    \n",
    "    # File size\n",
    "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    print(f\"\\nModel size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Mobile compatibility check\n",
    "    print(f\"\\nüì± Mobile Deployment Readiness:\")\n",
    "    print(f\"  ‚úÖ Fixed input size: {not any('unk' in str(dim) for dim in session.get_inputs()[0].shape)}\")\n",
    "    print(f\"  ‚úÖ ONNX opset ‚â§ 13: {model.opset_import[0].version <= 13}\")\n",
    "    print(f\"  ‚úÖ Model size < 50MB: {size_mb < 50}\")\n",
    "    print(f\"  ‚úÖ FP32 precision: {session.get_inputs()[0].type == 'tensor(float)'}\")\n",
    "\n",
    "# Analyze converted models\n",
    "if 'onnx_path' in locals() and os.path.exists(onnx_path):\n",
    "    analyze_onnx_model(onnx_path)\n",
    "\n",
    "if 'face_onnx_path' in locals() and os.path.exists(face_onnx_path):\n",
    "    analyze_onnx_model(face_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc336f",
   "metadata": {},
   "source": [
    "## 5. Mobile Deployment Guide\n",
    "\n",
    "### For Android:\n",
    "1. Use **ONNX Runtime Mobile** for Android\n",
    "2. Convert models to **ORT format** for even better mobile performance:\n",
    "   ```bash\n",
    "   python -m onnxruntime.tools.convert_onnx_models_to_ort ./\n",
    "   ```\n",
    "\n",
    "### For iOS:\n",
    "1. Use **ONNX Runtime** or **Core ML**\n",
    "2. For Core ML, convert ONNX to Core ML format:\n",
    "   ```python\n",
    "   import coremltools as ct\n",
    "   model = ct.convert('model.onnx')\n",
    "   model.save('model.mlmodel')\n",
    "   ```\n",
    "\n",
    "### Performance Tips:\n",
    "- Use **CPU optimized builds** of ONNX Runtime\n",
    "- Enable **parallel execution** for multi-core devices\n",
    "- Consider **quantization** for even smaller models\n",
    "- Profile on target device for optimal settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a4be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ONNX Conversion Summary\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully converted 2 model(s):\n",
      "  üì¶ models\\yolov8n-face-lindevs.onnx (11.56 MB)\n",
      "  üì¶ models\\yolov8n.onnx (12.15 MB)\n",
      "\n",
      "üì± Ready for mobile deployment!\n",
      "üí° Next steps:\n",
      "   1. Test models on your target mobile device\n",
      "   2. Optimize inference settings for your hardware\n",
      "   3. Consider further quantization if needed\n"
     ]
    }
   ],
   "source": [
    "# Generate summary of all converted models\n",
    "print(\"\\nüéâ ONNX Conversion Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "onnx_files = [f for f in os.listdir('.') if f.endswith('.onnx')]\n",
    "models_onnx_files = []\n",
    "if os.path.exists('models'):\n",
    "    models_onnx_files = [f for f in os.listdir('models') if f.endswith('.onnx')]\n",
    "\n",
    "all_onnx_files = onnx_files + [os.path.join('models', f) for f in models_onnx_files]\n",
    "\n",
    "if all_onnx_files:\n",
    "    print(f\"\\n‚úÖ Successfully converted {len(all_onnx_files)} model(s):\")\n",
    "    for file in all_onnx_files:\n",
    "        size = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"  üì¶ {file} ({size:.2f} MB)\")\n",
    "    \n",
    "    print(\"\\nüì± Ready for mobile deployment!\")\n",
    "    print(\"üí° Next steps:\")\n",
    "    print(\"   1. Test models on your target mobile device\")\n",
    "    print(\"   2. Optimize inference settings for your hardware\")\n",
    "    print(\"   3. Consider further quantization if needed\")\n",
    "else:\n",
    "    print(\"‚ùå No ONNX models found. Please run the conversion cells above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
